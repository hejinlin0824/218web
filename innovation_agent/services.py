import fitz  # PyMuPDF
from openai import OpenAI
from django.conf import settings
from .models import LLMConfiguration, InnovationProject, ProjectChatHistory
from .utils import EncryptionManager
from .prompts import PromptManager
import logging
import re  # ğŸ‘ˆ å¿…é¡»å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼åº“

logger = logging.getLogger(__name__)

class PDFProcessor:
    @staticmethod
    def extract_text(file_path):
        try:
            doc = fitz.open(file_path)
            text = ""
            for page in doc:
                text += page.get_text()
            return text
        except Exception as e:
            logger.error(f"PDF Parse Error: {e}")
            raise ValueError(f"PDF è§£æå¤±è´¥: {str(e)}")

class LLMService:
    def __init__(self, user):
        self.user = user
        self.config = self._get_config()
        self.client = self._init_client()

    def _get_config(self):
        try:
            return LLMConfiguration.objects.get(user=self.user)
        except LLMConfiguration.DoesNotExist:
            raise ValueError("è¯·å…ˆåœ¨ä¸ªäººä¸­å¿ƒé…ç½® AI æ¨¡å‹ API Key")

    def _init_client(self):
        raw_key = EncryptionManager().decrypt(self.config.encrypted_api_key)
        if not raw_key:
            raise ValueError("API Key è§£å¯†å¤±è´¥æˆ–æœªé…ç½®")
        return OpenAI(api_key=raw_key, base_url=self.config.base_url)

    def call_model(self, messages, project: InnovationProject = None):
        try:
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=messages,
                temperature=0.7,
                stream=False
            )
            content = response.choices[0].message.content
            
            if response.usage and project:
                total_usage = response.usage.total_tokens
                project.total_tokens_used += total_usage
                project.save(update_fields=['total_tokens_used'])
                
            return content
        except Exception as e:
            logger.error(f"LLM Call Error: {e}")
            raise ValueError(f"AI è°ƒç”¨å¤±è´¥: {str(e)}ï¼Œè¯·æ£€æŸ¥ Key æˆ–ä½™é¢")

# ==========================================
# ä¸šåŠ¡é€»è¾‘å°è£…
# ==========================================

def generate_baseline_summary(project_id, user):
    """Step 2: Baseline æ€»ç»“ (ç›´æ¥ç”Ÿæˆï¼Œè§†ä¸ºå·²å®šç¨¿è‰ç¨¿)"""
    project = InnovationProject.objects.get(id=project_id, user=user)
    
    # 1. è§£æ PDF
    pdf_path = project.baseline_file.path
    full_text = PDFProcessor.extract_text(pdf_path)
    
    ProjectChatHistory.objects.create(
        project=project, role='system',
        content=f"å·²è§£æ PDF ({len(full_text)}å­—ç¬¦)ï¼Œæ­£åœ¨ç”Ÿæˆ Baseline åˆ†æ..."
    )

    # 2. è°ƒç”¨ AI
    prompt_content = PromptManager.get_baseline_prompt(full_text)
    messages = [{"role": "user", "content": prompt_content}]
    llm = LLMService(user)
    summary = llm.call_model(messages, project=project)
    
    # 3. å¼ºåˆ¶ä¿å­˜ (Baseline ä¸éœ€è¦ Draft åè®®ï¼Œç›´æ¥è§†ä¸ºæ–‡æ¡£)
    project.base_md_content = summary
    project.status = 2 # è¿› Innov 1
    project.save()
    
    ProjectChatHistory.objects.create(
        project=project, role='assistant',
        content="âœ… **Baseline åˆ†æå®Œæˆ**ã€‚\n\nè¯·æŸ¥çœ‹å³ä¾§æ–‡æ¡£ã€‚ç°åœ¨å¼€å§‹æ„æ€ **åˆ›æ–°ç‚¹ 1**ã€‚"
    )
    
    return summary

def refine_innovation(project_id, user, user_idea, innov_index=1):
    """
    Step 3/4: åˆ›æ–°ç‚¹æ„æ€ (æ ¸å¿ƒé€»è¾‘å‡çº§)
    """
    project = InnovationProject.objects.get(id=project_id, user=user)
    
    # 1. è®°å½•ç”¨æˆ·è¾“å…¥
    ProjectChatHistory.objects.create(project=project, role='user', content=user_idea)

    # 2. ğŸ”¥ å…³é”®é€»è¾‘ï¼šæ„å»ºåŠ¨æ€ä¸Šä¸‹æ–‡ (Memory Construction) ğŸ”¥
    # å‘Šè¯‰ AI ä¹‹å‰å·²ç»ç¡®å®šäº†ä»€ä¹ˆï¼Œé˜²æ­¢å®ƒæå‡ºé‡å¤æˆ–å†²çªçš„æ–¹æ¡ˆ
    prev_innovs_context = "æš‚æ— å‰åºåˆ›æ–°ç‚¹"
    
    if innov_index == 2:
        # æ„æ€ç‚¹2æ—¶ï¼Œå¿…é¡»çŸ¥é“ç‚¹1æ˜¯ä»€ä¹ˆ
        prev_innovs_context = f"""
        [å·²é”å®šçš„ Innovation 1]:
        {project.innov1_md_content}
        """
    elif innov_index == 3:
        # æ„æ€ç‚¹3æ—¶ï¼Œå¿…é¡»çŸ¥é“ç‚¹1å’Œç‚¹2
        prev_innovs_context = f"""
        [å·²é”å®šçš„ Innovation 1]:
        {project.innov1_md_content}
        
        [å·²é”å®šçš„ Innovation 2]:
        {project.innov2_md_content}
        """

    # 3. è°ƒç”¨ AI (æ³¨å…¥å®Œæ•´ä¸Šä¸‹æ–‡)
    # æ³¨æ„ï¼šè¿™é‡Œè°ƒç”¨çš„æ˜¯æ–°ç‰ˆ PromptManagerï¼Œå®ƒä¼šè‡ªåŠ¨åˆ¤æ–­æ˜¯â€œä¸»åŠ¨å»ºè®®â€è¿˜æ˜¯â€œè¢«åŠ¨æ¶¦è‰²â€
    prompt = PromptManager.get_innovation_prompt(
        stage_num=innov_index,
        base_content=project.base_md_content, # è¿™é‡ŒåŒ…å«äº†å¯¹ Baseline çš„å®Œæ•´åæ§½
        prev_innovations=prev_innovs_context,
        user_idea=user_idea
    )

    llm = LLMService(user)
    # ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯ + ç”¨æˆ·æç¤ºè¯çš„ç»„åˆ
    raw_response = llm.call_model([
        {"role": "system", "content": PromptManager.CORE_SYSTEM_CONTEXT}, # æ³¨å…¥å®ªæ³•
        {"role": "user", "content": prompt}
    ], project=project)
    
    # 4. è§£æ <DRAFT> æ ‡ç­¾
    # (è¿™éƒ¨åˆ†é€»è¾‘ä¿æŒä¸å˜ï¼Œç”¨äºåˆ†ç¦»å¯¹è¯å’Œæ–‡æ¡£)
    draft_match = re.search(r'<DRAFT>(.*?)</DRAFT>', raw_response, re.DOTALL)
    
    response_data = {
        'chat_content': raw_response,
        'draft_content': None,
        'is_draft': False
    }

    if draft_match:
        draft_content = draft_match.group(1).strip()
        
        # è‡ªåŠ¨ä¿å­˜è‰ç¨¿åˆ°å¯¹åº”å­—æ®µ
        if innov_index == 1: project.innov1_md_content = draft_content
        elif innov_index == 2: project.innov2_md_content = draft_content
        elif innov_index == 3: project.innov3_md_content = draft_content
        project.save()
        
        response_data['is_draft'] = True
        response_data['draft_content'] = draft_content
        # ç§»é™¤æ ‡ç­¾ï¼Œåªæ˜¾ç¤ºèŠå¤©éƒ¨åˆ†
        chat_part = raw_response.replace(draft_match.group(0), "").strip()
        response_data['chat_content'] = chat_part if chat_part else "å·²ä¸ºæ‚¨ç”Ÿæˆè¯¦ç»†æ–¹æ¡ˆæ–‡æ¡£ï¼Œè¯·åœ¨å³ä¾§æŸ¥çœ‹å¹¶ç¡®è®¤ã€‚"

    # 5. è®°å½• AI å›å¤
    ProjectChatHistory.objects.create(
        project=project, role='assistant',
        content=response_data['chat_content']
    )
    
    return response_data

def confirm_innovation(project_id, user, content, innov_index):
    """ç”¨æˆ·ç‚¹å‡»â€œå®šç¨¿â€æ—¶è°ƒç”¨"""
    project = InnovationProject.objects.get(id=project_id, user=user)
    current_status = project.status
    
    # æ›´æ–°å¯¹åº”å­—æ®µ (è™½ç„¶è‰ç¨¿å·²ç»å­˜äº†ï¼Œä½†è¿™é‡Œæ˜¯æœ€ç»ˆç¡®è®¤ï¼Œå¯èƒ½åœ¨å‰ç«¯æ”¹è¿‡)
    if current_status == 2:
        project.innov1_md_content = content
        project.status = 3
    elif current_status == 3:
        project.innov2_md_content = content
        project.status = 4
    elif current_status == 4:
        project.innov3_md_content = content
        project.status = 5
    elif current_status == 5:
        project.exp_md_content = content
        project.status = 6
        
    project.save()

def generate_experiment_design(project_id, user):
    """Step 5: å®éªŒè®¾è®¡ (é€šå¸¸åŒ…å« DRAFT)"""
    project = InnovationProject.objects.get(id=project_id, user=user)
    
    ProjectChatHistory.objects.create(project=project, role='user', content="ç”Ÿæˆå®éªŒè®¾è®¡")

    prompt = PromptManager.get_experiment_prompt(
        project.base_md_content, 
        project.innov1_md_content, 
        project.innov2_md_content, 
        project.innov3_md_content
    )
    
    llm = LLMService(user)
    raw_response = llm.call_model([{"role": "user", "content": prompt}], project=project)
    
    # è§£æ DRAFT
    draft_match = re.search(r'<DRAFT>(.*?)</DRAFT>', raw_response, re.DOTALL)
    
    response_data = {
        'chat_content': raw_response,
        'draft_content': None,
        'is_draft': False
    }
    
    if draft_match:
        draft_content = draft_match.group(1).strip()
        project.exp_md_content = draft_content
        project.save() # å­˜è‰ç¨¿
        
        response_data['is_draft'] = True
        response_data['draft_content'] = draft_content
        
        chat_part = raw_response.replace(draft_match.group(0), "").strip()
        if not chat_part: chat_part = "å®éªŒæ–¹æ¡ˆå·²ç”Ÿæˆï¼Œè¯·æ£€æŸ¥ã€‚"
        response_data['chat_content'] = chat_part
    
    ProjectChatHistory.objects.create(
        project=project, role='assistant',
        content=response_data['chat_content']
    )
    
    return response_data