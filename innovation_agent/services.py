import fitz  # PyMuPDF
from openai import OpenAI
from django.conf import settings
from .models import LLMConfiguration, InnovationProject, ProjectChatHistory
from .utils import EncryptionManager
from .prompts import PromptManager
import logging
import re  # ğŸ‘ˆ å¿…é¡»å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼åº“

logger = logging.getLogger(__name__)

class PDFProcessor:
    @staticmethod
    def extract_text(file_path):
        try:
            doc = fitz.open(file_path)
            text = ""
            for page in doc:
                text += page.get_text()
            return text
        except Exception as e:
            logger.error(f"PDF Parse Error: {e}")
            raise ValueError(f"PDF è§£æå¤±è´¥: {str(e)}")

class LLMService:
    def __init__(self, user):
        self.user = user
        self.config = self._get_config()
        self.client = self._init_client()

    def _get_config(self):
        try:
            return LLMConfiguration.objects.get(user=self.user)
        except LLMConfiguration.DoesNotExist:
            raise ValueError("è¯·å…ˆåœ¨ä¸ªäººä¸­å¿ƒé…ç½® AI æ¨¡å‹ API Key")

    def _init_client(self):
        raw_key = EncryptionManager().decrypt(self.config.encrypted_api_key)
        if not raw_key:
            raise ValueError("API Key è§£å¯†å¤±è´¥æˆ–æœªé…ç½®")
        return OpenAI(api_key=raw_key, base_url=self.config.base_url)

    def call_model(self, messages, project: InnovationProject = None):
        try:
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=messages,
                temperature=0.7,
                stream=False
            )
            content = response.choices[0].message.content
            
            if response.usage and project:
                total_usage = response.usage.total_tokens
                project.total_tokens_used += total_usage
                project.save(update_fields=['total_tokens_used'])
                
            return content
        except Exception as e:
            logger.error(f"LLM Call Error: {e}")
            raise ValueError(f"AI è°ƒç”¨å¤±è´¥: {str(e)}ï¼Œè¯·æ£€æŸ¥ Key æˆ–ä½™é¢")

# ==========================================
# ä¸šåŠ¡é€»è¾‘å°è£…
# ==========================================

def generate_baseline_summary(project_id, user):
    """Step 2: Baseline æ€»ç»“ (ç›´æ¥ç”Ÿæˆï¼Œè§†ä¸ºå·²å®šç¨¿è‰ç¨¿)"""
    project = InnovationProject.objects.get(id=project_id, user=user)
    
    # 1. è§£æ PDF
    pdf_path = project.baseline_file.path
    full_text = PDFProcessor.extract_text(pdf_path)
    
    ProjectChatHistory.objects.create(
        project=project, role='system',
        content=f"å·²è§£æ PDF ({len(full_text)}å­—ç¬¦)ï¼Œæ­£åœ¨ç”Ÿæˆ Baseline åˆ†æ..."
    )

    # 2. è°ƒç”¨ AI
    prompt_content = PromptManager.get_baseline_prompt(full_text)
    messages = [{"role": "user", "content": prompt_content}]
    llm = LLMService(user)
    summary = llm.call_model(messages, project=project)
    
    # 3. å¼ºåˆ¶ä¿å­˜ (Baseline ä¸éœ€è¦ Draft åè®®ï¼Œç›´æ¥è§†ä¸ºæ–‡æ¡£)
    project.base_md_content = summary
    project.status = 2 # è¿› Innov 1
    project.save()
    
    ProjectChatHistory.objects.create(
        project=project, role='assistant',
        content="âœ… **Baseline åˆ†æå®Œæˆ**ã€‚\n\nè¯·æŸ¥çœ‹å³ä¾§æ–‡æ¡£ã€‚ç°åœ¨å¼€å§‹æ„æ€ **åˆ›æ–°ç‚¹ 1**ã€‚"
    )
    
    return summary

def refine_innovation(project_id, user, user_idea, innov_index=1):
    """Step 3/4: åˆ›æ–°ç‚¹æ„æ€ (æ ¸å¿ƒï¼šæ”¯æŒèŠå¤© vs è‰ç¨¿åè®®)"""
    project = InnovationProject.objects.get(id=project_id, user=user)
    
    # 1. è®°å½•ç”¨æˆ·è¾“å…¥
    ProjectChatHistory.objects.create(project=project, role='user', content=user_idea)

    # 2. å‡†å¤‡ä¸Šä¸‹æ–‡
    prev_innovs = ""
    if innov_index > 1: prev_innovs += f"\n--- Innov 1 ---\n{project.innov1_md_content}\n"
    if innov_index > 2: prev_innovs += f"\n--- Innov 2 ---\n{project.innov2_md_content}\n"
    if not prev_innovs: prev_innovs = "æ— "

    # 3. è°ƒç”¨ AI
    prompt = PromptManager.get_innovation_prompt(innov_index, project.base_md_content, prev_innovs, user_idea)
    llm = LLMService(user)
    raw_response = llm.call_model([{"role": "user", "content": prompt}], project=project)
    
    # 4. ğŸ”¥ è§£æ <DRAFT> æ ‡ç­¾ ğŸ”¥
    draft_match = re.search(r'<DRAFT>(.*?)</DRAFT>', raw_response, re.DOTALL)
    
    response_data = {
        'chat_content': raw_response, # é»˜è®¤æ˜¾ç¤ºå…¨éƒ¨
        'draft_content': None,
        'is_draft': False
    }

    if draft_match:
        # A. æå–åˆ°è‰ç¨¿ -> å­˜åº“ï¼Œå‰ç«¯å¼¹çª—
        draft_content = draft_match.group(1).strip()
        
        # å­˜å…¥æ•°æ®åº“å¯¹åº”å­—æ®µ (ä½œä¸ºè‰ç¨¿)
        if innov_index == 1: project.innov1_md_content = draft_content
        elif innov_index == 2: project.innov2_md_content = draft_content
        elif innov_index == 3: project.innov3_md_content = draft_content
        project.save()
        
        response_data['is_draft'] = True
        response_data['draft_content'] = draft_content
        
        # èŠå¤©æ¡†åªæ˜¾ç¤º <DRAFT> ä¹‹å¤–çš„å¼•å¯¼è¯­
        chat_part = raw_response.replace(draft_match.group(0), "").strip()
        if not chat_part: chat_part = "å·²ä¸ºä½ ç”Ÿæˆè¯¦ç»†æ–¹æ¡ˆè‰ç¨¿ï¼Œè¯·åœ¨å³ä¾§æŸ¥çœ‹ã€‚"
        response_data['chat_content'] = chat_part
        
    else:
        # B. æ²¡æå–åˆ° -> çº¯èŠå¤©æ¨¡å¼
        response_data['is_draft'] = False
        # ä¸å­˜åº“ï¼Œåªè®°å½•èŠå¤©å†å²
    
    # 5. è®°å½• AI å›å¤åˆ°å†å²
    ProjectChatHistory.objects.create(
        project=project, role='assistant',
        content=response_data['chat_content']
    )
    
    return response_data

def confirm_innovation(project_id, user, content, innov_index):
    """ç”¨æˆ·ç‚¹å‡»â€œå®šç¨¿â€æ—¶è°ƒç”¨"""
    project = InnovationProject.objects.get(id=project_id, user=user)
    current_status = project.status
    
    # æ›´æ–°å¯¹åº”å­—æ®µ (è™½ç„¶è‰ç¨¿å·²ç»å­˜äº†ï¼Œä½†è¿™é‡Œæ˜¯æœ€ç»ˆç¡®è®¤ï¼Œå¯èƒ½åœ¨å‰ç«¯æ”¹è¿‡)
    if current_status == 2:
        project.innov1_md_content = content
        project.status = 3
    elif current_status == 3:
        project.innov2_md_content = content
        project.status = 4
    elif current_status == 4:
        project.innov3_md_content = content
        project.status = 5
    elif current_status == 5:
        project.exp_md_content = content
        project.status = 6
        
    project.save()

def generate_experiment_design(project_id, user):
    """Step 5: å®éªŒè®¾è®¡ (é€šå¸¸åŒ…å« DRAFT)"""
    project = InnovationProject.objects.get(id=project_id, user=user)
    
    ProjectChatHistory.objects.create(project=project, role='user', content="ç”Ÿæˆå®éªŒè®¾è®¡")

    prompt = PromptManager.get_experiment_prompt(
        project.base_md_content, 
        project.innov1_md_content, 
        project.innov2_md_content, 
        project.innov3_md_content
    )
    
    llm = LLMService(user)
    raw_response = llm.call_model([{"role": "user", "content": prompt}], project=project)
    
    # è§£æ DRAFT
    draft_match = re.search(r'<DRAFT>(.*?)</DRAFT>', raw_response, re.DOTALL)
    
    response_data = {
        'chat_content': raw_response,
        'draft_content': None,
        'is_draft': False
    }
    
    if draft_match:
        draft_content = draft_match.group(1).strip()
        project.exp_md_content = draft_content
        project.save() # å­˜è‰ç¨¿
        
        response_data['is_draft'] = True
        response_data['draft_content'] = draft_content
        
        chat_part = raw_response.replace(draft_match.group(0), "").strip()
        if not chat_part: chat_part = "å®éªŒæ–¹æ¡ˆå·²ç”Ÿæˆï¼Œè¯·æ£€æŸ¥ã€‚"
        response_data['chat_content'] = chat_part
    
    ProjectChatHistory.objects.create(
        project=project, role='assistant',
        content=response_data['chat_content']
    )
    
    return response_data